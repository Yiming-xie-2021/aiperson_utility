{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMln+I6nrTHtt7+ejZQ216T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yiming-xie-2021/aiperson_utility/blob/main/colab_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link Google Drive and download model to drive, so can fast load from drive."
      ],
      "metadata": {
        "id": "e0I4Manyhz0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# link drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# download a specific model to the drive\n",
        "if False:\n",
        "  !git lfs install\n",
        "  !git clone https://huggingface.co/unsloth/Qwen3-1.7B-unsloth-bnb-4bit /content/drive/MyDrive/models/Qwen3-1.7B-unsloth-bnb-4bit\n",
        "\n",
        "# list of models under the path\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/models/\"\n",
        "for f in os.listdir(path):\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05SyeYSTePNN",
        "outputId": "45f09674-63fb-48a2-eb61-1615b1c0aabd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Qwen3-1.7B-unsloth-bnb-4bit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "make sure GPU is on"
      ],
      "metadata": {
        "id": "e7gkOOJejAlK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69b0fa6f"
      },
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install -q langchain langchain-community"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "# import torch\n",
        "\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
        "#     max_seq_length = 2048,\n",
        "#     load_in_4bit = True,\n",
        "#     load_in_8bit = False,\n",
        "#     full_finetuning = False,\n",
        "# )\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/models/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U33VpvuOMkCS",
        "outputId": "a5061bc6-df32-45e9-9062-68fdf29148f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.6: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"æ±Ÿæ³½æ°‘æ˜¯è°ï¼Œä¹ è¿‘å¹³åˆæ˜¯è°ï¼Œè°¢ä¸€é¸£åˆæ˜¯è°ï¼Œä»–ä»¬ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ.\"}\n",
        "  ]\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = False,\n",
        "      add_generation_prompt = True,\n",
        "      enable_thinking = False\n",
        "  )\n",
        "\n",
        "  from transformers import TextStreamer\n",
        "  _ = model.generate(\n",
        "      **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 2000,\n",
        "      temperature = 0.7, top_p = 0.8, top_k = 20,\n",
        "      streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "  )"
      ],
      "metadata": {
        "id": "ki2R_QVGMoYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain langchain-community\n",
        "# å¯¼å…¥ LangChain çš„ LLM åŸºç±»\n",
        "from langchain_core.language_models import LLM\n",
        "from typing import Optional, List, Any\n",
        "\n",
        "class MyHFLLM(LLM):\n",
        "    # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "    model: Any\n",
        "    tokenizer: Any\n",
        "    enable_thinking: bool = False\n",
        "\n",
        "    def _call(self, prompt: str, stop=None):\n",
        "        # æŠŠ LangChainä¼ è¿›æ¥çš„ prompt è½¬æ¢æˆ messages\n",
        "        messages = [\n",
        "            # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=self.enable_thinking)\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=2000)\n",
        "        gen_tokens = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
        "        return self.tokenizer.decode(gen_tokens[0], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self):\n",
        "        # ç”¨äº LangChain å†…éƒ¨æ ‡è¯†\n",
        "        return {\"model\": self.model.__class__.__name__}\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self):\n",
        "        # è‡ªå®šä¹‰ç±»å‹æ ‡è¯†\n",
        "        return \"custom_hf\"\n",
        "\n",
        "\n",
        "# ---------------- ä½¿ç”¨ç¤ºä¾‹ ----------------\n",
        "\n",
        "# å‡è®¾ä½ å·²ç»åŠ è½½å¥½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œä¾‹å¦‚ï¼š\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"ä½ çš„æ¨¡å‹è·¯å¾„\").to(\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"ä½ çš„æ¨¡å‹è·¯å¾„\")\n",
        "\n",
        "# é»˜è®¤ä¸å¯ç”¨æ€ç»´æ¨¡å¼\n",
        "llm = MyHFLLM(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# å¯ç”¨æ€ç»´æ¨¡å¼\n",
        "llm_thinking = MyHFLLM(model=model, tokenizer=tokenizer, enable_thinking=True)\n",
        "\n",
        "# å®šä¹‰ LangChain çš„ Prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "# æŠŠ Prompt å’Œè‡ªå®šä¹‰ LLM ç»„åˆæˆä¸€ä¸ªé“¾\n",
        "chain = prompt | llm\n",
        "\n",
        "# æµ‹è¯•è°ƒç”¨\n",
        "print(chain.invoke({\"question\": \"è°æ˜¯æ±Ÿæ³½æ°‘ï¼Ÿ\"}))\n"
      ],
      "metadata": {
        "id": "y2yOlO0Vkbh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_thinking = prompt | llm_thinking\n",
        "print(chain_thinking.invoke({\"question\": \"\"}))"
      ],
      "metadata": {
        "id": "Tw3LSUjGC7xi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}